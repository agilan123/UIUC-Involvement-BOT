{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZxGz-M9Rsr7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#assuming, we've already trained the models, and the models are available and utilizable in the filesystem -- let's put the address of where the files are in! That was we can use them.\n",
        "CATEGORICAL_CLASSIFIER_MODEL_PATH = 'categorical_classifier.h5'\n",
        "INTENSITY_PREDICTOR_MODEL_PATH = 'intensity_predictor.h5'\n",
        "\n",
        "#Okay we have two models, one to predict pre-determined categories, and on to predict intensity of the queries.\n",
        "categorical_classifier = keras.models.load_model(CATEGORICAL_CLASSIFIER_MODEL_PATH)\n",
        "intensity_predictor = keras.models.load_model(INTENSITY_PREDICTOR_MODEL_PATH)\n",
        "\n",
        "\n",
        "#This just serves to tokenize each query, make each query the same length as the other queries, remove punctuation, etc.\n",
        "def preprocess_query(query, tokenizer, max_length=100):\n",
        "    #use the trained tokenizer, provided as a parameter in the function.\n",
        "    sequences = tokenizer.texts_to_sequences([query])\n",
        "    #pad the sequences to the right length.\n",
        "    padded = pad_sequences(sequences, maxlen=max_length)\n",
        "    return padded\n",
        "\n",
        "\n",
        "def predict_category_and_intensity(json_input):\n",
        "    #load in the data from the JSON. We have JSON, but we want the string (the actual query) that the data represents\n",
        "    data = json.loads(json_input)\n",
        "    query = data['query']\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=30000) #Since we are generating about 1000 queries, we can assume 30-ish words per query? If each word (worst-case) is distinct, the tokenizer needs a dictionary of 30000 words.\n",
        "\n",
        "    #let's call the function above!\n",
        "    processed_query = preprocess_query(query, tokenizer)\n",
        "\n",
        "    #below, we are passing the query into both models, taking the output, storing the predicted category in cateogry_pred and the predicted intensity in intensity_pred.\n",
        "    category_pred = categorical_classifier.predict(processed_query), axis\n",
        "    intensity_pred = np.argmax(intensity_predictor.predict(processed_query), axis=-1)\n",
        "\n",
        "    #We are returning a list of {x, y} where x is the predicted cateogry, and y is the predicted intesity\n",
        "    return {\n",
        "        \"category_prediction\": str(category_pred),\n",
        "        \"intensity_prediction\": int(intensity_pred)\n",
        "    }\n"
      ]
    }
  ]
}