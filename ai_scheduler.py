# -*- coding: utf-8 -*-
"""AI_Scheduler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12gjDnUo3UuyKWSIex4LIHbY4nmUHTGY7
"""

import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense

df = pd.read_csv('Shuffled_ML_training_scheduler.csv')
queries = df['query'].values
priorities = df['priority'].values

X_train, X_test, y_train, y_test = train_test_split(queries, priorities, test_size=0.2, random_state=42)

tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

max_length = max(len(x) for x in X_train_seq)
X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

priority_model = Sequential([
    Embedding(input_dim=10000, output_dim=16, input_length=max_length),
    GlobalAveragePooling1D(),
    Dense(24, activation='relu'),
    Dense(4, activation='softmax')
])

priority_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
priority_model.summary()


history = priority_model.fit(X_train_padded, y_train, epochs=10, validation_data=(X_test_padded, y_test))


test_loss, test_acc = priority_model.evaluate(X_test_padded, y_test, verbose=2)
print('\nTest accuracy:', test_acc)


predictions = priority_model.predict(X_test_padded)
predicted_priorities = np.argmax(predictions, axis=1)





priority_model.save('priority_model.h5')

new_query = ["I have a doctors appointment sometime this week"]
sequence = tokenizer.texts_to_sequences(new_query)


padded_sequence = pad_sequences(sequence, maxlen=100, padding='post')  # Use the same maxlen as during training


prediction = model.predict(padded_sequence)

predicted_priority = np.argmax(prediction, axis=1)
print(f"Predicted Priority: {predicted_priority[0]}")

queries = df['query'].values
intensities = df['intensity'].values  # Assuming 'intensity' column exists

# Split the data
X_train, X_test, y_train, y_test = train_test_split(queries, intensities, test_size=0.2, random_state=42)

tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)


max_length = max(len(x) for x in X_train_seq)  # Or set a predefined max_length
X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# Adjust the model to fit the intensity prediction
# The last Dense layer might need adjustments based on the range of your intensity values
model = Sequential([
    Embedding(input_dim=10000, output_dim=16, input_length=max_length),
    GlobalAveragePooling1D(),
    Dense(24, activation='relu'),
    Dense(1, activation='linear')  # For intensity, you might use a single neuron with linear activation
])

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])  # Use MSE and MAE for regression tasks
model.summary()

# Train the model
history = model.fit(X_train_padded, y_train, epochs=10, validation_data=(X_test_padded, y_test))

# Evaluate the model
test_loss, test_mae = model.evaluate(X_test_padded, y_test, verbose=2)
print('\nTest MAE:', test_mae)

# Making predictions
predictions = model.predict(X_test_padded)
# For intensity, you might directly use the predictions without needing to apply argmax since it's a regression task now

# Assuming `tokenizer` and `model` are already defined and properly trained

# Example query
query = "I have a doctors appointment"

# Tokenize and pad the query
sequence = tokenizer.texts_to_sequences([query])
padded_sequence = pad_sequences(sequence, maxlen=100, padding='post')  # Ensure maxlen matches training setup

# Predict intensity
predicted_intensity = model.predict(padded_sequence)

# Assuming intensity is a single regression output from the model
print(f"Predicted Intensity: {predicted_intensity[0][0]}")